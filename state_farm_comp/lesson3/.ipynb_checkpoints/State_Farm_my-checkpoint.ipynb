{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what am I doing here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the steps:\n",
    "    - Load the data\n",
    "    - Set-up my machine learning environment\n",
    "    - Organize the data into an appropriate directory structure (mimic what he has been doing) with training, validation and test sets (making the right validation set will possibly be tricky on this dataset\n",
    "    - Read the data into the needed data structures for learning (I still really need to learn how to do this)\n",
    "    - Start with a linear model\n",
    "    - Then start doing multilayer MLP\n",
    "    - Then start using the pre-trained network, pre-compute convolutional layers, train fully connected layers\n",
    "    - Start applying the techniques that Jeremy Howard has been teaching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mlesson3\u001b[0m/  \u001b[01;34mutils\u001b[0m/\n",
      "downloading https://www.kaggle.com/c/state-farm-distracted-driver-detection/download/sample_submission.csv.zip\n",
      "\n",
      "sample_submission.csv.zip N/A% |                      | ETA:  --:--:--   0.0 s/B\n",
      "\n",
      "Warning: download url for file sample_submission.csv.zip resolves to an html document rather than a downloadable file. \n",
      "Is it possible you have not accepted the competition's rules on the kaggle website?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create references to important directories we will use over and over\n",
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow relative imports to directories above lesson1/\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../utils/'))\n",
    "\n",
    "#import modules\n",
    "from utils import *; \n",
    "from vgg16 import Vgg16\n",
    "from IPython.display import FileLink\n",
    "\n",
    "#Instantiate plotting tool\n",
    "#In Jupyter notebooks, you will need to run this command before doing any plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs-git/state_farm_comp/lesson3/data\n",
      "downloading https://www.kaggle.com/c/state-farm-distracted-driver-detection/download/sample_submission.csv.zip\n",
      "\n",
      "sample_submission.csv.zip 100% |#####################| Time: 0:00:00 383.5 KiB/s\n",
      "\n",
      "downloading https://www.kaggle.com/c/state-farm-distracted-driver-detection/download/imgs.zip\n",
      "\n",
      "imgs.zip 100% |######################################| Time: 0:02:28  27.7 MiB/s\n",
      "\n",
      "downloading https://www.kaggle.com/c/state-farm-distracted-driver-detection/download/driver_imgs_list.csv.zip\n",
      "\n",
      "driver_imgs_list.csv.zip 100% |######################| Time: 0:00:00 238.9 KiB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Data\n",
    "%cd $DATA_HOME_DIR\n",
    "!kg download -u remi10001 -p $KAGGLE_PASS -c state-farm-distracted-driver-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decompress data\n",
    "!unzip -q imgs.zip\n",
    "!unzip -q driver_imgs_list.csv.zip\n",
    "!rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs-git/state_farm_comp/lesson3/data\n"
     ]
    }
   ],
   "source": [
    "#Create directories\n",
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test/unknown\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "#%mkdir -p test/unknown\n",
    "%mkdir tester\n",
    "%mv test/ tester/unknown\n",
    "%mv tester test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Validation set, train and validation separate drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/ubuntu/nbs-git/state_farm_comp/lesson3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs-git/state_farm_comp/lesson3/data\n",
      "classname   c0   c1   c2   c3   c4   c5   c6   c7   c8   c9\n",
      "subject                                                    \n",
      "p002        76   74   86   79   84   76   83   72   44   51\n",
      "p012        84   95   91   89   97   96   75   72   62   62\n",
      "p014       100  103  100  100  103  102  101   77   38   52\n",
      "p015        79   85   88   94  101  101   99   81   86   61\n",
      "p016       111  102  101  128  104  104  108  101   99  120\n",
      "p021       135  131  127  128  132  130  126   98   99  131\n",
      "p022       129  129  128  129  130  130  131   98   98  131\n",
      "p024       130  129  128  130  129  131  129  101   99  120\n",
      "p026       130  129  130  131  126  130  128   97   97   98\n",
      "p035        94   81   88   89   89   89   94   87   56   81\n",
      "p039        65   63   70   65   62   64   63   64   70   65\n",
      "p041        60   64   60   60   60   61   61   61   59   59\n",
      "p042        59   59   60   59   58   59   59   59   59   60\n",
      "p045        75   75   76   75   75   76   71   67   66   68\n",
      "p047        80   91   81   86   82   87   81   82   82   83\n",
      "p049        84   85  119  110  109  116  119   74   79  116\n",
      "p050       123   45   52   98   83   91   82   81   65   70\n",
      "p051       182   81   81   83   81   83   95   80   62   92\n",
      "p052        72   71   84   75   72   72   77   71   71   75\n",
      "p056        81   80   80   78   82   81   80   74   83   75\n",
      "p061        84   81   81   83   79   81   80   79   81   80\n",
      "p064        83   81   83   84   86   85   82   79   81   76\n",
      "p066       129  100  106  101  102  101  105   86  114   90\n",
      "p072        63   62   36   31   34    6   35    2   21   56\n",
      "p075        81   81   85   79   89   79   82   82   79   77\n",
      "p081       100   90   96   82   77   81   79   77   61   80\n",
      "subject\n",
      "p002     725\n",
      "p012     823\n",
      "p014     876\n",
      "p015     875\n",
      "p016    1078\n",
      "p021    1237\n",
      "p022    1233\n",
      "p024    1226\n",
      "p026    1196\n",
      "p035     848\n",
      "p039     651\n",
      "p041     605\n",
      "p042     591\n",
      "p045     724\n",
      "p047     835\n",
      "p049    1011\n",
      "p050     790\n",
      "p051     920\n",
      "p052     740\n",
      "p056     794\n",
      "p061     809\n",
      "p064     820\n",
      "p066    1034\n",
      "p072     346\n",
      "p075     814\n",
      "p081     823\n",
      "dtype: int64\n",
      "Validation subjects: p075, p050, p052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c0/img_68359.jpg', 'c0/img_35502.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess class distribution for the subjects in the training dataset\n",
    "%cd $DATA_HOME_DIR\n",
    "import pandas as pd\n",
    "# Read in the data table\n",
    "a = pd.read_csv('driver_imgs_list.csv')\n",
    "\n",
    "tab = a.groupby(['subject', 'classname']).size()\n",
    "print(tab.unstack())\n",
    "tab = a.groupby('subject').size()\n",
    "print(tab)\n",
    "\n",
    "# select three subjects at random for the validation set\n",
    "import random\n",
    "random.seed(100)   # So subjects selected are consistent\n",
    "b =set(np.random.permutation(a['subject']))\n",
    "subs_val = random.sample(b - set('p072'), 3)# Decided on 3 drivers with further consultation from Jeremy Howard's notebook\n",
    "print(\"Validation subjects: \" + ', '.join(subs_val))\n",
    "\n",
    "a['val.file'] = a[['classname', 'img']].apply(lambda x: '/'.join(x), axis=1)\n",
    "tab_val = a.loc[a['subject'].isin(subs_val)]\n",
    "val_files =tab_val['val.file'].tolist()\n",
    "val_files[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the separate directories in valid and sample/train\n",
    "os.chdir(DATA_HOME_DIR+'/train')\n",
    "for d in glob('c?'):\n",
    "    os.mkdir('../sample/train/' + d)\n",
    "    os.mkdir('../sample/valid/' + d)\n",
    "    os.mkdir('../valid/' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now move the files from the training folder to the validation folder\n",
    "os.chdir(DATA_HOME_DIR + '/train')\n",
    "from shutil import copyfile\n",
    "for f in val_files:\n",
    "    os.rename(f, DATA_HOME_DIR + '/valid/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now copy files to sample folders\n",
    "#How many do I want in my sample? maybe 50 train, 20 valid, 20 test [just to make sure code is working]\n",
    "train_sample = 50\n",
    "valid_sample = 20\n",
    "test_sample = 20\n",
    "\n",
    "os.chdir(DATA_HOME_DIR + '/train')\n",
    "fs = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(fs)\n",
    "for i in range(train_sample):\n",
    "    copyfile(shuf[i], '../sample/train/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(DATA_HOME_DIR + '/valid')\n",
    "fs = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(fs)\n",
    "for i in range(valid_sample):\n",
    "    copyfile(shuf[i], '../sample/valid/' + shuf[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(DATA_HOME_DIR + '/test/unknown')\n",
    "fs = glob('*.jpg')\n",
    "shuf = np.random.permutation(fs)\n",
    "for i in range(test_sample):\n",
    "    copyfile(shuf[i], '../../sample/test/unknown/' + shuf[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now assume I have the data in all the directories I want. Now what do I need to do? One approach is that I can make a simple linear model in keras. \n",
    "    - I need to get batches of my training data and validation data. I can use the get batches function for that. \n",
    "    - Then I can define a sequential linear model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs-git/state_farm_comp/lesson3/data\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "#Set path to sample/ path if desired\n",
    "path = DATA_HOME_DIR + '/' #'/sample/'\n",
    "\n",
    "test_path = path + '/test/' \n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + '/train/'\n",
    "valid_path=path + '/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 10 classes.\n",
      "Found 20 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??get_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 10 classes.\n",
      "Found 20 images belonging to 10 classes.\n",
      "Found 20 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, \n",
    " train_filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 1s - loss: 9.7120 - acc: 0.1200 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s - loss: 10.9603 - acc: 0.3200 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 1s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s - loss: 11.2827 - acc: 0.3000 - val_loss: 15.3122 - val_acc: 0.0500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0901f6250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_batches.nb_sample, nb_epoch=10, \n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict_generator(train_batches, train_batches.nb_sample)[:10], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly always predicting 1 of 2 classes. Decrease the learning rate to see if it will do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 1s - loss: 3.6615 - acc: 0.1600 - val_loss: 14.6681 - val_acc: 0.0500\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 1s - loss: 2.5600 - acc: 0.1800 - val_loss: 10.4166 - val_acc: 0.0500\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s - loss: 2.1767 - acc: 0.3600 - val_loss: 8.3791 - val_acc: 0.0500\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s - loss: 1.8068 - acc: 0.4600 - val_loss: 7.3171 - val_acc: 0.0500\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s - loss: 1.5889 - acc: 0.5000 - val_loss: 6.8109 - val_acc: 0.0500\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s - loss: 1.1698 - acc: 0.6400 - val_loss: 6.4534 - val_acc: 0.0500\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s - loss: 1.0534 - acc: 0.6600 - val_loss: 6.4669 - val_acc: 0.0500\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s - loss: 0.9361 - acc: 0.7200 - val_loss: 6.4526 - val_acc: 0.1000\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s - loss: 0.8546 - acc: 0.7400 - val_loss: 5.9429 - val_acc: 0.1000\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s - loss: 0.6641 - acc: 0.8600 - val_loss: 5.5745 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff08d11d910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-5),\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_batches, train_batches.nb_sample, nb_epoch=10, \n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150528"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 224 * 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 150528 pixels. Thus, with 10 hidden units, the model can overfit the training data as much as it wants. That is what I see, ever increasing accuracy on the training set but no increase in accuracy on the validation set beyond chance (predicting the same class for all samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead straight to the Vgg16 pre-trained model and fine-tune it. If that takes too long, I can pre-compute the convolutional layers and then train the other layers. After I do this, I can go back and look through all the steps that he did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 22s - loss: 4.3720 - acc: 0.1400 - val_loss: 4.5390 - val_acc: 0.0500\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 21s - loss: 4.9361 - acc: 0.1400 - val_loss: 4.0187 - val_acc: 0.0500\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 21s - loss: 3.2543 - acc: 0.3600 - val_loss: 3.2506 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 21s - loss: 2.8068 - acc: 0.3200 - val_loss: 2.8694 - val_acc: 0.2000\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 21s - loss: 3.3810 - acc: 0.2200 - val_loss: 3.1717 - val_acc: 0.1500\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 21s - loss: 2.6494 - acc: 0.3000 - val_loss: 3.0018 - val_acc: 0.1500\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 21s - loss: 1.7781 - acc: 0.4400 - val_loss: 2.7893 - val_acc: 0.1500\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 21s - loss: 2.0502 - acc: 0.5000 - val_loss: 2.9055 - val_acc: 0.2000\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 21s - loss: 1.5123 - acc: 0.5600 - val_loss: 3.0757 - val_acc: 0.0500\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 21s - loss: 1.6710 - acc: 0.4600 - val_loss: 2.8468 - val_acc: 0.2000\n"
     ]
    }
   ],
   "source": [
    "vgg_model = Vgg16()\n",
    "vgg_model.finetune(train_batches)\n",
    "vgg_model.fit(train_batches, val_batches, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned Vgg model on this amount of data can start to fit the training data and at least get to 20% validation accuracy.\n",
    "I am using a vanishingly small amount of the data, so this is probably not a good test of these parameters, and pre-trained model architecture. IT IS EXCITING TO ME HOW SIMPLE THE VGG16 CODE NOW LOOKS. It is starting to make a lot of sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think now I should start going through Jeremy Howard's approach. Try to write the code myself after looking at his, or especially just after considering his thoughts. I am working towards having my own thoughts on how to improve accuracy and being able to implement those thoughts through keras/theano directly, rather than through Jeremy's code. I can't rely on him as a crutch, though he is certainly helping me to learn things faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My approach to good performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to recreate what Jeremy Howard did in his State Farm submission, and I will build on it. I will use the pre-trained Vgg16 network, with data augmentation, and dropout and batch normalization on the dense layers. I will use the same network that he used, basically. I will train this model with sequential lowering of learning rate on the validation set. Once performance stabilizes I will PREDICT ON THE TEST SET. I am going to do a 5x larger training set with data augmentation. And I will do a 2x larger test set with data augmentation after I have my predictions (Actually, right now I only do 1x test set, I'll keep at that for now). Then I will continue to refine the model that I used (with the saved weights from the first time around). I should be able to get all of this code working on the CPU before I do it on the GPU, simply using my samples. The experiments won't be of any consequence.\n",
    "\n",
    "At each step I need to save weights as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 10 classes.\n",
      "Found 20 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "#train_batches = get_batches(train_path, batch_size=batch_size, shuffle=False)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(test_path, batch_size=batch_size, shuffle=False)\n",
    "#conv_feat = conv_model.predict_generator(train_batches, train_batches.nb_sample)\n",
    "conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 10 classes.\n",
      "Found 20 images belonging to 10 classes.\n",
      "Found 20 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, \n",
    " train_filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_array(path + \"results/conv_feat.dat\", conv_feat)\n",
    "save_array(path + \"results/conv_val_feat.dat\", conv_val_feat)\n",
    "save_array(path + \"results/conv_test_feat.dat\", conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv_feat = load_array(path + \"results/conv_feat.dat\")\n",
    "conv_val_feat = load_array(path + \"results/conv_val_feat.dat\")\n",
    "conv_test_feat = load_array(path + \"results/conv_test_feat.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05,\n",
    "                                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "da_batches= get_batches(train_path, gen_t, batch_size=batch_size, shuffle=False)\n",
    "da_conv_feat = conv_model.predict_generator(da_batches, da_batches.nb_sample*5)\n",
    "save_array(path + \"results/da_conv_feat2.dat\", da_conv_feat)\n",
    "da_conv_feat = load_array(path + \"results/da_conv_feat2.dat\")\n",
    "\n",
    "# Include the real training data too in unaugmented form\n",
    "da_conv_feat = np.concatenate([da_conv_feat, conv_feat])\n",
    "da_trn_labels = np.concatenate([trn_labels]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_layers(p=0.8):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape = conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 20 samples\n",
      "Epoch 1/2\n",
      "300/300 [==============================] - 0s - loss: 4.9148 - acc: 0.1100 - val_loss: 9.6294 - val_acc: 0.1000\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 0s - loss: 4.5590 - acc: 0.1367 - val_loss: 7.6492 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3df46ad9d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0.8\n",
    "da_dis_model = Sequential(get_my_layers(p))\n",
    "da_dis_model.compile(optimizer=Adam(lr=0.001),\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "da_dis_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "                    validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 0s - loss: 4.1791 - acc: 0.1500 - val_loss: 6.1278 - val_acc: 0.1000\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s - loss: 4.2040 - acc: 0.1000 - val_loss: 4.7814 - val_acc: 0.1500\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s - loss: 3.9867 - acc: 0.1633 - val_loss: 4.1615 - val_acc: 0.2500\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s - loss: 3.7041 - acc: 0.1767 - val_loss: 4.0150 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s - loss: 3.4227 - acc: 0.2067 - val_loss: 3.8159 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e1b5a41d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_dis_model.optimizer.lr = 0.01\n",
    "\n",
    "da_dis_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=5, \n",
    "                    validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 20 samples\n",
      "Epoch 1/8\n",
      "300/300 [==============================] - 0s - loss: 3.3517 - acc: 0.2567 - val_loss: 3.7103 - val_acc: 0.2000\n",
      "Epoch 2/8\n",
      "300/300 [==============================] - 0s - loss: 2.8911 - acc: 0.2533 - val_loss: 3.4906 - val_acc: 0.2000\n",
      "Epoch 3/8\n",
      "300/300 [==============================] - 0s - loss: 3.0908 - acc: 0.2333 - val_loss: 3.2785 - val_acc: 0.2500\n",
      "Epoch 4/8\n",
      "300/300 [==============================] - 0s - loss: 2.8503 - acc: 0.2600 - val_loss: 3.1403 - val_acc: 0.2500\n",
      "Epoch 5/8\n",
      "300/300 [==============================] - 0s - loss: 2.9236 - acc: 0.2700 - val_loss: 3.0291 - val_acc: 0.2500\n",
      "Epoch 6/8\n",
      "300/300 [==============================] - 0s - loss: 2.8150 - acc: 0.2800 - val_loss: 2.9226 - val_acc: 0.2500\n",
      "Epoch 7/8\n",
      "300/300 [==============================] - 0s - loss: 2.6581 - acc: 0.3167 - val_loss: 2.7990 - val_acc: 0.2500\n",
      "Epoch 8/8\n",
      "300/300 [==============================] - 0s - loss: 2.7476 - acc: 0.3167 - val_loss: 2.6770 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3df46ad210>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_dis_model.optimizer.lr = 0.0001\n",
    "\n",
    "da_dis_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=8, \n",
    "                    validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_dis_model.save_weights(path + \"results/da_dis_conv_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporate Test Data in Semi-Supervised Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pseudo = da_dis_model.predict(conv_test_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([da_trn_labels, test_pseudo])\n",
    "comb_feat = np.concatenate([da_conv_feat, conv_test_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the model with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_dis_model.load_weights(path + \"results/da_dis_conv_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "320/320 [==============================] - 0s - loss: 2.3960 - acc: 0.3500 - val_loss: 2.6138 - val_acc: 0.2000\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - 0s - loss: 2.1930 - acc: 0.3594 - val_loss: 2.5626 - val_acc: 0.2000\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - 0s - loss: 2.3292 - acc: 0.3750 - val_loss: 2.4993 - val_acc: 0.2000\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - 0s - loss: 2.1089 - acc: 0.4594 - val_loss: 2.4248 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "320/320 [==============================] - 0s - loss: 2.0822 - acc: 0.4438 - val_loss: 2.3335 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3df46adcd0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_dis_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=5, \n",
    "                    validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 20 samples\n",
      "Epoch 1/8\n",
      "320/320 [==============================] - 0s - loss: 2.0062 - acc: 0.4375 - val_loss: 2.2898 - val_acc: 0.2000\n",
      "Epoch 2/8\n",
      "320/320 [==============================] - 0s - loss: 1.8676 - acc: 0.4594 - val_loss: 2.2431 - val_acc: 0.2000\n",
      "Epoch 3/8\n",
      "320/320 [==============================] - 0s - loss: 1.7249 - acc: 0.5312 - val_loss: 2.2445 - val_acc: 0.2000\n",
      "Epoch 4/8\n",
      "320/320 [==============================] - 0s - loss: 1.8181 - acc: 0.4719 - val_loss: 2.2745 - val_acc: 0.2000\n",
      "Epoch 5/8\n",
      "320/320 [==============================] - 0s - loss: 1.5383 - acc: 0.5406 - val_loss: 2.2657 - val_acc: 0.2000\n",
      "Epoch 6/8\n",
      "320/320 [==============================] - 0s - loss: 1.5347 - acc: 0.5312 - val_loss: 2.2971 - val_acc: 0.2000\n",
      "Epoch 7/8\n",
      "320/320 [==============================] - 0s - loss: 1.5802 - acc: 0.5156 - val_loss: 2.3150 - val_acc: 0.2000\n",
      "Epoch 8/8\n",
      "320/320 [==============================] - 0s - loss: 1.4438 - acc: 0.5531 - val_loss: 2.2461 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3dea01eb90>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_dis_model.optimizer.lr=0.00001\n",
    "da_dis_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=8, \n",
    "                    validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_dis_model.save_weights(path + \"results/da_dis_conv_model_pred_test.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path + \"results/conv_test_feat.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = da_dis_model.predict(conv_test_feat, batch_size=batch_size*2)\n",
    "subm = do_clip(preds, 0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_13614.jpg</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.044560</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.691862</td>\n",
       "      <td>0.073427</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.008953</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.069226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_8824.jpg</td>\n",
       "      <td>0.049301</td>\n",
       "      <td>0.007914</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.209866</td>\n",
       "      <td>0.653461</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.025065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_99787.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.011521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_41021.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.019544</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.017532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_85693.jpg</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.098793</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.028098</td>\n",
       "      <td>0.741938</td>\n",
       "      <td>0.014637</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.036738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img        c0        c1        c2        c3        c4        c5  \\\n",
       "0  img_13614.jpg  0.036290  0.044560  0.034003  0.020036  0.691862  0.073427   \n",
       "1   img_8824.jpg  0.049301  0.007914  0.007778  0.209866  0.653461  0.035907   \n",
       "2  img_99787.jpg  0.007778  0.007778  0.007778  0.007778  0.930000  0.008458   \n",
       "3  img_41021.jpg  0.007778  0.007778  0.007778  0.007778  0.930000  0.019544   \n",
       "4  img_85693.jpg  0.036370  0.098793  0.011236  0.028098  0.741938  0.014637   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.018919  0.008953  0.007778  0.069226  \n",
       "1  0.007778  0.007778  0.007778  0.025065  \n",
       "2  0.007778  0.007778  0.007778  0.011521  \n",
       "3  0.007778  0.007778  0.007778  0.017532  \n",
       "4  0.018100  0.010490  0.007778  0.036738  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_name = path + \"results/subm.gz\"\n",
    "classes = sorted(train_batches.class_indices, key=train_batches.class_indices.get)\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/nbs-git/state_farm_comp/lesson3/data/sample/results/subm.gz' target='_blank'>/home/ubuntu/nbs-git/state_farm_comp/lesson3/data/sample/results/subm.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/nbs-git/state_farm_comp/lesson3/data/sample/results/subm.gz"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
